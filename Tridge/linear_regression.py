# -*- coding: utf-8 -*-
"""tridge.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-Mp_bKB0clZ4zPs45eJAylhf7yyTK7vQ
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn import preprocessing
import seaborn as sns
from scipy.stats import norm, skew

from google.colab import drive
drive.mount("/content/gdrive")

path = '/content/gdrive/MyDrive/Colab Notebooks'
file_path = os.path.join(path, 'house_prices.xlsx')

df = pd.read_excel(file_path)

df.shape

df.head()

# Our target column is the price column
print(df['House unit price'].describe())

"""# Feature Analysis
Useful if we have a lot of features
"""

df['Transaction']=df['Transaction'].astype('category').cat.codes
df['House age']=df['House age'].astype('category').cat.codes
df['Distance to MRT station']=df['Distance to MRT station'].astype('category').cat.codes
df['Number of convenience stores']=df['Number of convenience stores'].astype('category').cat.codes
df['Latitude']=df['Latitude'].astype('category').cat.codes
df['Longitude']=df['Longitude'].astype('category').cat.codes

df.corr()

# What are most important features that affect house prices

corrMatrix = df.corr()
f, ax = plt.subplots(figsize=(12, 9))
sns.heatmap(corrMatrix, vmax=.8, square=True);

# Keeping only the important features

corr = df.corr()
highest_corr_features = corr.index[abs(corr['House unit price']) > 0.40] | corr.index[abs(corr['House unit price']) < -0.40]
plt.figure(figsize=(10,10))
g = sns.heatmap(df[highest_corr_features].corr(),annot=True,cmap="RdYlGn")

print(highest_corr_features)

df = df[highest_corr_features]
id = np.array(range(0,414))
df = df.assign(ID=id)

df.head()

"""# Looking for missing data"""

Total = df.isnull().sum().sort_values(ascending=False)
percent = (df.isnull().sum() / df.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([Total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(25)

"""No missing data found.

# Feature Engineering
"""

# Fix the skewness in other features
features = df.dtypes[df.dtypes != 'object'].index
skewed_features = df[features].apply(lambda x: skew(x)).sort_values(ascending=False)
high_skew = skewed_features[abs(skewed_features) > 0.5]
high_skew

for feature in high_skew.index:
    df[feature] = np.log1p(df[feature])

"""# Separating the data"""

from sklearn.model_selection import train_test_split
df_train, df_test = train_test_split(df, test_size=0.2)

df_train.shape, df_test.shape

x_train = df_train.drop(['ID', 'House unit price'], axis=1)
y_train = df_train['House unit price']

x_train.shape, y_train.shape

x_test = df_test.drop(['ID', 'House unit price'], axis=1)
y_test = df_test['House unit price']
test_id = df_test['ID']

x_test.shape, y_test.shape

"""# Training model"""

from sklearn.metrics import make_scorer
from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression

model = LinearRegression()

model.fit(x_train, y_train)

print(model.score(x_test, y_test))

"""# Prediction"""

y_pred = np.floor(np.expm1(model.predict(x_test)))
y_pred

y_test = df_test['House unit price']
y_test.head()

y_test = np.floor(np.expm1(y_test))
y_test

# Error
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("RMSE: %f" % (rmse))

sub = pd.DataFrame()
sub['ID'] = test_id
sub['House unit price'] = y_pred
sub.to_csv(os.path.join(path, 'mysubmission.csv'),index=False)

